# GQA

多查询注意力（MQA）仅使用单个键值头，可大幅加速解码器推理。然而MQA可能导致质量下降，且仅为加快推理而单独训练模型未必可取。我们（1）提出一种方案，可将现有多头语言模型检查点上训练为MQA模型，其所需预训练计算量仅为原始模型的5%；（2）引入分组查询注意力（GQA）——这是多查询注意力的泛化形式，采用中间数量（大于1且小于查询头总数）的键值头。
实验表明，上调训练后的GQA模型在质量上接近多头注意力模型，同时达到与MQA相当的运行速度。

1 引言

自回归解码器推理是Transformer模型的严重瓶颈，其根源在于每次解码步骤中加载解码器权重及所有注意力键值对所产生的内存带宽开销（Shazeer, 2019; Pope et al., 2022; de Jong et al., 2022）。通过多查询注意力机制（Shazeer, 2019）可显著降低加载键值对的内存带宽消耗——该机制采用多查询头但仅使用单键值头。

然而多查询注意力（MQA）可能导致质量下降与训练不稳定，且分别训练优化质量与推断效率的模型往往不可行。此外，尽管部分语言模型（如PaLM（Chowdhery等人，2022））已采用多查询注意力机制，但多数模型（包括T5（Raffel等人，2020）和LLaMA（Touvron等人，2023）等公开模型）尚未应用该技术。

本研究为大型语言模型加速推理提供了两项创新：首先，我们证明通过多头注意力（MHA）训练的语言模型检查点可采用上训练技术（Komatsuzaki et al., 2022），仅需少量原始训练计算资源即可启用多查询注意力。这为获取高速多查询功能与高质量MHA检查点提供了成本效益高的解决方案。

其次，我们提出分组查询注意力机制（GQA），该机制在多头注意力与多查询注意力之间进行插值，通过为每个查询头子组配置独立的键值头实现优化。实验表明，上训练后的GQA在质量上接近多头注意力，同时几乎达到多查询注意力的运行速度。

2 方法

2.1 上行训练

从多头模型生成多查询模型分为两个步骤：首先转换检查点，其次进行额外预训练以使模型适应新结构。图1展示了将多头检查点转换为多查询检查点的流程。通过均值池化将键头和值头的投影矩阵合并为单一投影矩阵，我们发现该方法优于单独选取键头/值头或从零随机初始化新键值头。

转换后的检查点需在原始预训练步骤的α比例范围内，采用相同预训练方案进行补充训练。

2.2 分组查询注意力机制

分组查询注意力机制将查询头划分为G个组，每组共享单个键头与值头。其中GQA-G指具有G个组的分组查询机制。GQA-1（单组结构）仅含单个键值头，等效于MQA；而GQA-H（组数等于头部数量）等效于MHA。图2展示了分组查询注意力与多头/多查询注意力的对比。在将多头检查点转换为GQA检查点时，我们通过对组内所有原始头部进行均值池化，构建每个组的键头和值头。中等数量的组可生成插值模型，其质量优于MQA但速度快于MHA，且如后续所示，实现了理想的权衡。从MHA转向MQA时，将H个键值头简化为单个键值头，使键值缓存规模缩减H倍，从而减少H倍的数据加载量。但大型模型通常会扩展头部数量，因此多查询注意力在内存带宽和容量方面实现了更激进的削减。GQA使带宽与容量的缩减比例能随模型规模增长保持恒定。

此外，大型模型受注意力机制内存带宽开销的影响相对较小——因为KV缓存随模型维度扩展，而模型浮点运算量和参数规模则随模型维度平方增长。最后，针对大型模型的标准分片方案会将单键值头复制为模型分区数量（Pope等，2022）；而GQA消除了此类分区造成的浪费。因此我们预期GQA能为大型模型提供尤为优越的权衡方案。

需说明的是，编码器自注意力层未应用GQA；编码器表示通过并行计算生成，内存带宽通常并非主要瓶颈。





请用通俗易懂的语言解释以下内容：

层归一化（LayerNorm）因其能同时处理输入与权重矩阵的重新归一化和重新缩放，已被成功应用于各类深度神经网络，有助于稳定训练并加速模型收敛。然而，LayerNorm引入的计算开销使得这些改进代价高昂，并显著拖慢底层网络运行速度，尤其对循环神经网络（RNN）影响更为显著。本文提出假设：LayerNorm中的重新居中不变性并非必需，并由此创立均方根层归一化（RMSNorm）。该方法通过均方根（RMS）对单层神经元输入总和进行正则化，赋予模型缩放不变性与隐含的学习率自适应能力。相较于LayerNorm，RMSNorm计算更简洁且效率更高。我们同时提出部分RMS归一化（pRMSNorm），其通过取p%输入总和估算RMS值，同时保持上述特性。在多种任务和网络架构上的广泛实验表明，RMSNorm在保持与LayerNorm相当的性能同时，不同模型上运行时间缩短7%至64%。
源代码详见：https://github.com/bzhangGo/rmsnorm

1 引言
如何高效训练深度神经网络始终是长期存在的挑战。为加速模型收敛，Ba等[3]提出了层归一化（LayerNorm）技术，通过利用均值与方差统计量对单层神经元动态进行正则化，从而稳定深度神经网络的训练过程。由于其简单易用且无需依赖训练样本间关联性，LayerNorm已被广泛应用于不同神经网络架构，在计算机视觉[19, 26]、语音识别[37]及自然语言处理[31, 35]等任务中取得显著成效。某些情况下，LayerNorm甚至被证实是成功训练模型的关键要素[6]。此外，与基于批次的样本解耦特性使LayerNorm在处理基于循环神经网络（RNN）的变长序列时，相较于批量归一化（BatchNorm）[12]更具优势。
遗憾的是，LayerNorm的引入增加了计算开销。对于规范层较少的小型浅层神经网络，此问题尚可忽略，但当底层网络规模扩大、深度增加时，该问题便变得严重。因此，虽然LayerNorm能通过加快训练速度和提升稳定性（以训练步数衡量）提高效率，但每训练步的计算成本增加抵消了这一优势，导致净效率下降，如图
图1所示。LayerNorm被广泛认为能稳定模型的关键特性之一是其重新居中不变性：当输入或权重矩阵偏移一定噪声量时，经LayerNorm处理后的输入总和保持不变。我们认为这种均值归一化并不能降低隐藏层状态或模型梯度的方差，并推测其对LayerNorm成功作用影响甚微。
本文提出均方根层归一化（RMSNorm），该方法仅通过均方根（RMS）统计量对单层神经元的输入总和进行正则化。

RMSNorm相较于LayerNorm可减少计算量并提升效率。尽管公式更为简洁，RMS归一化器仍能有效稳定层激活值的幅度，确保其对权重与数据集重缩放的不变性。我们还证明了仅对输入总和子集进行RMS估计仍能保持该不变性。假设输入总和具有独立同分布结构，我们提出部分RMS归一化方案：仅利用前p%的输入总和进行RMS估计。
我们通过机器翻译、图像分类、图像-标题检索及问答等任务对模型进行了全面验证。实验结果表明：在不同模型中，RMSNorm与LayerNorm性能相当，但运行速度提升7%∼64%。当采用部分（6.25%）累加输入进行RMS估计时，pRMSNorm表现出与RMSNorm相当的竞争力。

3 背景
本节基于标准前馈神经网络简要回顾LayerNorm。给定输入向量x ∈ R^m，前馈网络通过线性变换后接非线性激活函数将其投影为输出向量y ∈ R^n，过程如下：
其中 wi为第 i 个输出神经元的权重向量，bi为偏置标量（通常初始化为 0），f(·) 为逐元素非线性函数。a ∈ Rn 表示神经元权重加总后的输入，即归一化目标。
这种基础网络可能存在内部协变量偏移问题[12]，即随着前层更新导致某层输入分布发生变化。这会破坏参数梯度的稳定性，延迟模型收敛。为减少偏移，LayerNorm通过规范化求和输入来固定其均值与方差：
其中 a¯i是向量 ¯a ∈ Rn 的第 i 个值，作为层激活中 ai 的归一化替代量。g ∈ Rn 是用于重新缩放标准化总和输入的增益参数，初始值设为 1。µ 和 σ分别是根据原始输入总和 a 估算的均值和方差统计量：
因此，LayerNorm 强制使神经元的范数与输入和权重矩阵解耦。

请用通俗易懂的语言解释以下内容：

4 RMSNorm

LayerNorm成功的一个广为人知的解释在于其重新居中与重新缩放的不变性特性。前者使模型对输入和权重上的位移噪声不敏感，后者则在输入和权重被随机缩放时保持输出表示不变。本文提出假设：层归一化的成功源于缩放不变性而非重新归一化不变性。我们提出RMSNorm，该方法仅关注缩放不变性，并通过均方根(RMS)统计量对输入求和进行正则化：a¯i =aiRMS(a)gi，其中RMS(a) =vuut1nXni=1a2i。(4) 该方法通过完全移除公式(3)中的均值统计量来简化LayerNorm，代价是牺牲了均值归一化带来的不变性。当输入总和均值为零时，RMSNorm与LayerNorm完全等价。尽管RMSNorm不像LayerNorm那样重新对齐输入中心，但实验表明该特性并非LayerNorm成功的关键，且RMSNorm具有同等甚至更优的效能。

均方根范数衡量输入的二次均值，在RMSNorm中将输入总和强制映射至pn缩放的单位球面。此机制使输出分布不受输入与权重分布缩放的影响，从而增强层激活的稳定性。尽管仅与均方根范数存在pn系数差异的欧几里得范数已被成功应用[22]，但我们实证发现其不适用于层归一化。我们推测，使用输入向量尺寸对球体进行缩放至关重要，因为这能增强不同尺寸向量间的归一化鲁棒性。据我们所知，将均方根用于神经网络归一化的思路此前尚未被研究。

4.1 不变性分析

不变性衡量归一化后模型输出随输入与权重矩阵变化的程度。Ba等[3]指出不同归一化方法展现出差异化的不变性特征，这对模型鲁棒性具有显著影响。本节将从理论层面探讨RMSNorm的不变性属性。

我们考虑RMSNorm的通用形式：

y = f ⊙ RMS Wx(a) + b

其中⊙表示元素级乘法。主要结果见表1。由于RMS的线性特性：

RMS(αx) = αRMS(x)；

RMS(αx) = αRMS(x)； (6)

其中α为缩放因子。若权重矩阵按缩放因子δ调整（即W0 = δW），则该变换不影响最终层输出：

y0= f RMS W0(xa0) ⊙ g + b= f δRMS δWx(a) ⊙ g + b= y: (7)

相反地，若仅对单个权重向量进行缩放，该性质不再成立，因为不同的缩放因子会破坏RMS的线性特性。类似地，若对输入施加缩放因子δ，即x0 = δx，通过与式(7)类似的分析可得RMSNorm的输出保持不变。

该等式可轻松推广至批量输入及整个数据集。因此，RMSNorm对输入缩放具有不变性。

其与LayerNorm的核心差异在于：RMSNorm不进行重新归一化，故不具备变量偏移时的线性特性，且对所有重新归一化操作均不保持不变性。

4.2 梯度分析

上述分析仅考虑输入缩放与权重矩阵对层输出的影响。但在实际应用中，采用RMSNorm增强的神经网络需通过标准随机梯度下降法训练，此时模型梯度的鲁棒性对参数更新与模型收敛至关重要（参见Santurkar等[23]的论述：归一化方法的成功并非源于增强层输入稳定性，而是优化空间平滑度的提升）。本节将深入探究RMSNorm中模型梯度的特性。

给定损失函数 L，我们通过式(4)进行反向传播，以获得参数 g 和 b 的梯度如下：

RMS(a); (8)
其中 v 表示方程 (4) 中 f(·) 内整个表达式的简写，@L=@v 是从 L 反向传播至 v 的梯度。两个梯度 @L=@b 和 @L=@g 均不受输入 x 及权重矩阵 W 的缩放影响（@L=@g 的情况源于方程 (6) 的线性特性）。此外，g的梯度与归一化输入总和成正比，而非原始输入。这赋予了g幅值的稳定性。
不同于这些向量参数，权重矩阵W的梯度因均方根中的二次运算而更为复杂。形式上，
@L
@W = ⊗ nXi ⊗ xT ⊗ diag(·) g ⊙ @@Lv × R i ; 其中 R = RMS 1(a) ⊙ I − (Wx nRMS ) (Wx (a)2)T ; (9)
diag(·)表示输入的对角矩阵，⊗表示克罗内克积，"I"表示单位矩阵。为清晰起见，我们明确使用"×"表示矩阵乘法。矩阵项R将W的梯度与输入x和权重矩阵W相关联。通过深入分析，我们可以证明该项与输入和权重矩阵的缩放均呈负相关。
在将缩放因子δ应用于输入x（x0 = δx）或权重矩阵（W0 = δW）后，我们得到：
R0 = 1
δRMS(a) ∠ I − (δnδ Wx 2RMS ) (δWx (a)2)T ∠ = 1δR: (10)
若将缩放项R0代入式(9)，可轻易证明梯度@L=@W对输入缩放不变，但与权重矩阵缩放保持负相关。降低梯度 @L=@W 对输入缩放的敏感性，可确保其平滑性并提升学习稳定性。另一方面，负相关性作为隐式学习率调节器，动态控制梯度范数，既避免了权重矩阵范数过大，又改善了模型收敛性。
