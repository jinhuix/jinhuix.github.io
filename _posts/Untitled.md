请用通俗易懂的语言解释以下内容：

多查询注意力（MQA）仅使用单个键值头，可大幅加速解码器推理。然而MQA可能导致质量下降，且仅为加快推理而单独训练模型未必可取。我们（1）提出一种方案，可将现有多头语言模型检查点上训练为MQA模型，其所需预训练计算量仅为原始模型的5%；（2）引入分组查询注意力（GQA）——这是多查询注意力的泛化形式，采用中间数量（大于1且小于查询头总数）的键值头。
实验表明，上调训练后的GQA模型在质量上接近多头注意力模型，同时达到与MQA相当的运行速度。

1 引言

自回归解码器推理是Transformer模型的严重瓶颈，其根源在于每次解码步骤中加载解码器权重及所有注意力键值对所产生的内存带宽开销（Shazeer, 2019; Pope et al., 2022; de Jong et al., 2022）。通过多查询注意力机制（Shazeer, 2019）可显著降低加载键值对的内存带宽消耗——该机制采用多查询头但仅使用单键值头。

然而多查询注意力（MQA）可能导致质量下降与训练不稳定，且分别训练优化质量与推断效率的模型往往不可行。此外，尽管部分语言模型（如PaLM（Chowdhery等人，2022））已采用多查询注意力机制，但多数模型（包括T5（Raffel等人，2020）和LLaMA（Touvron等人，2023）等公开模型）尚未应用该技术。

本研究为大型语言模型加速推理提供了两项创新：首先，我们证明通过多头注意力（MHA）训练的语言模型检查点可采用上训练技术（Komatsuzaki et al., 2022），仅需少量原始训练计算资源即可启用多查询注意力。这为获取高速多查询功能与高质量MHA检查点提供了成本效益高的解决方案。

其次，我们提出分组查询注意力机制（GQA），该机制在多头注意力与多查询注意力之间进行插值，通过为每个查询头子组配置独立的键值头实现优化。实验表明，上训练后的GQA在质量上接近多头注意力，同时几乎达到多查询注意力的运行速度。

2 方法

2.1 上行训练

从多头模型生成多查询模型分为两个步骤：首先转换检查点，其次进行额外预训练以使模型适应新结构。图1展示了将多头检查点转换为多查询检查点的流程。通过均值池化将键头和值头的投影矩阵合并为单一投影矩阵，我们发现该方法优于单独选取键头/值头或从零随机初始化新键值头。

转换后的检查点需在原始预训练步骤的α比例范围内，采用相同预训练方案进行补充训练。

2.2 分组查询注意力机制

分组查询注意力机制将查询头划分为G个组，每组共享单个键头与值头。其中GQA-G指具有G个组的分组查询机制。GQA-1（单组结构）仅含单个键值头，等效于MQA；而GQA-H（组数等于头部数量）等效于MHA。图2展示了分组查询注意力与多头/多查询注意力的对比。在将多头检查点转换为GQA检查点时，我们通过对组内所有原始头部进行均值池化，构建每个组的键头和值头。中等数量的组可生成插值模型，其质量优于MQA但速度快于MHA，且如后续所示，实现了理想的权衡。从MHA转向MQA时，将H个键值头简化为单个键值头，使键值缓存规模缩减H倍，从而减少H倍的数据加载量。但大型模型通常会扩展头部数量，因此多查询注意力在内存带宽和容量方面实现了更激进的削减。GQA使带宽与容量的缩减比例能随模型规模增长保持恒定。

此外，大型模型受注意力机制内存带宽开销的影响相对较小——因为KV缓存随模型维度扩展，而模型浮点运算量和参数规模则随模型维度平方增长。最后，针对大型模型的标准分片方案会将单键值头复制为模型分区数量（Pope等，2022）；而GQA消除了此类分区造成的浪费。因此我们预期GQA能为大型模型提供尤为优越的权衡方案。

需说明的是，编码器自注意力层未应用GQA；编码器表示通过并行计算生成，内存带宽通常并非主要瓶颈。



